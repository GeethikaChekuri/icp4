{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KDM_ICP4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjcwzS24Jr_"
      },
      "source": [
        "Setting up PySpark in Colab\n",
        "\n",
        "Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iGswToj4LIS"
      },
      "source": [
        "!sudo apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzRtNPkq5VIR"
      },
      "source": [
        "Next, we will install Apache Spark 3.0.1 with Hadoop 2.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBYZfbpk34H3"
      },
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L21HeCi5bde"
      },
      "source": [
        "Now, we just need to unzip that folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-D2QoWK5cEg"
      },
      "source": [
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIssZqC8aYj"
      },
      "source": [
        "There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wke-MaxX57M9"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark==3.0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AJ1RV578hE9"
      },
      "source": [
        "Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMEW0ol59ts"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] =  \"/content/spark-3.0.3-bin-hadoop3.2\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weGRBVLK8nKb"
      },
      "source": [
        "We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSleoMKz6Ecg"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gW5avs8yWa"
      },
      "source": [
        "If you want to know the location where Spark is installed, use findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a3EHL6x76KcK",
        "outputId": "8807be5e-7050-40e5-d2d8-a847c0d87c86"
      },
      "source": [
        "findspark.find()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.0.3-bin-hadoop3.2'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI0EI6Ox87wT"
      },
      "source": [
        "Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.\n",
        "\n",
        "You can give a name to the session using appName() and add some configurations with config() if you wish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcGOvWMJ6PUo"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djWQ29kL9Ogy"
      },
      "source": [
        "Finally, print the SparkSession variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvcENqE16XEj"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4btlV8p6b3Q"
      },
      "source": [
        "#we need to load the dataset. We will use the read.csv module. \n",
        "#The inferSchema parameter provided will enable Spark to automatically determine the data type for each column but it has to go over the data once.\n",
        "# If you donâ€™t want that to happen, then you can instead provide the schema explicitly in the schema parameter.\n",
        "\n",
        "df = spark.read.csv(\"/content/data.csv\", header=True, inferSchema= True)\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0U-Tk7V7NC5"
      },
      "source": [
        "df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZkvWbw97S7Y"
      },
      "source": [
        "#If you didn't set inderShema to True, here is what is happening to the type. There are all in string.\n",
        "df_string = spark.read.csv(\"/content/data.csv\", header=True, inferSchema=  False)\n",
        "df_string.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mgfvQEo74_Y"
      },
      "source": [
        "#You can select and show the rows with select and the names of the features. Below, gender and churn are selected.\n",
        "df.select('gender','churn').show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-aXM9_x8BIU"
      },
      "source": [
        "#To get a summary statistics, of the data, you can use describe(). It will compute the :count, mean, standarddeviation, min, max\n",
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccQW1Roy97p7"
      },
      "source": [
        "#Distinct values for Categorical columns\n",
        "#The distinct() will come in handy when you want to determine the unique values in the categorical columns in the dataframe.\n",
        "\n",
        "df.select(\"PaymentMethod\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zmVWSmOEqaB"
      },
      "source": [
        "Map reduce example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qMnKKNQ-mPZ"
      },
      "source": [
        "# For map reduce we need to create SparkContext so that we can read a textfile and perform mapping function on it. \n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "text_file = sc.textFile(\"/content/icp4.txt\")\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "for x in counts.collect():\n",
        "    print (x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actions"
      ],
      "metadata": {
        "id": "u26nynkpFKAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "Cru250mcFHzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Finding First Row"
      ],
      "metadata": {
        "id": "Be682Rn3FWtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.first()"
      ],
      "metadata": {
        "id": "F16ujvWaFXnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Required rows using take function\n"
      ],
      "metadata": {
        "id": "S2VOhsonFeJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.take(5)"
      ],
      "metadata": {
        "id": "JDl0CPAjFefz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().toPandas().transpose()"
      ],
      "metadata": {
        "id": "c9fT7vAWIBGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(df.take(5), columns=df.columns)"
      ],
      "metadata": {
        "id": "fyxhxFqqIIAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('InternetService').take(5)#action on next block "
      ],
      "metadata": {
        "id": "yPr1XTv-IObP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('InternetService').first()"
      ],
      "metadata": {
        "id": "MvBtbxXwIY-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Collect:This task helps us present the PaymentMethod column's elements as an array to the drivernode\n",
        "\n",
        "df.select('PaymentMethod').collect()"
      ],
      "metadata": {
        "id": "dfoafGdcNMkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting Null records in the partner column\n",
        "\n",
        "df.filter(df[\"partner\"].isNull()).count()"
      ],
      "metadata": {
        "id": "X8bZTBLtNY3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Min: using an aggregate function min, i will display the minimum monthly charge on the df\n",
        "\n",
        "df.agg({\"MonthlyCharges\":\"min\",\"MonthlyCharges\":\"min\"}).show()"
      ],
      "metadata": {
        "id": "9VxvvOGDNiHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get last 3 element using tail function\n",
        "\n",
        "action = df.tail(3)\n",
        "print(\"action: Getting last 3 data points from data set\")\n",
        "action"
      ],
      "metadata": {
        "id": "wxo1LyavPBBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing summary\n",
        "\n",
        "act = df.summary()\n",
        "print(\"action: Getting all the summary of data set\")\n",
        "act.show()"
      ],
      "metadata": {
        "id": "sOmkcfBkPK1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tranformations**"
      ],
      "metadata": {
        "id": "zXFKWucOFu4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the data\n"
      ],
      "metadata": {
        "id": "wfEFij1LF0F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace('Male',None).show()"
      ],
      "metadata": {
        "id": "b6nqt7xkFvIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DZRvuu3NG_mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"nullValue\", \"NA\").option(\"timestampFormat\", \"yyyy-MM-dd'T'HH:mm?:ss\").option(\"mode\", \"failfast\").option(\n",
        "    \"path\",\n",
        "    \"/content/data.csv\"\n",
        "  ).load()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "Y7OWSMdDG_8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding required data using filter\n"
      ],
      "metadata": {
        "id": "F8d6nE4bF9hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.filter(df.gender == 'Male').collect()"
      ],
      "metadata": {
        "id": "Vpxs9JdiF9wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping the data and showing count.\n"
      ],
      "metadata": {
        "id": "XCj6FYFgGFkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "op=df.groupBy('gender').count()\n",
        "op.show()\n"
      ],
      "metadata": {
        "id": "bfqZYYKdGFz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lambda function is a small anonymous function.A lambda function can take any number of arguments, but can only have one expression.The power of lambda is better shown when you use them as an anonymous function inside another function.\n",
        "\n",
        "The truncate() method resizes the file to the given number of bytes."
      ],
      "metadata": {
        "id": "CaqQ0pqWHn8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "\n",
        "binary_map = {'Yes':1.0, 'No':0.0, 'True':1.0, 'False':0.0}\n",
        "toNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())\n",
        "\n",
        "df = df.drop('customerID').withColumn('Churn', toNum(df['Churn'])).withColumn('Partner', toNum(df['Partner'])).withColumn('Dependents', toNum(df['Dependents'])).cache()\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "y9mCptTiHoXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = sc.textFile(\"/content/data.csv\").map(lambda line : line.split(\",\"))\n",
        "#Filter function : Return a new dataset formed by selecting those elements of the source on which func returns true.\n",
        "#ex: customer ID ; 7795-CFOCW\n",
        "data1 = file.filter(lambda line: \"One year\" in line).count()\n",
        "print(data1)"
      ],
      "metadata": {
        "id": "jIarxnbMIriT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAP function = applies transformation function on dataset and returns same number of elements in distributed dataset.\n",
        "\n",
        "fm = file.map(lambda column : (column [20], 1)).reduceByKey(lambda x, y: x + y)\n",
        "fm.collect()"
      ],
      "metadata": {
        "id": "TTsAu2-nI-IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here i am using the filter function to show only information for those who pay with electronic check and have one year contracts\n",
        "\n",
        "df.filter(\"PaymentMethod='Electronic check' and contract='One year'\").show()"
      ],
      "metadata": {
        "id": "ljAI8l-5KC5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Groupby: telling spark to group my data by customer ID and internetservice and i am getting the maximum charge for each internetservice and customer that pays for it\n",
        "\n",
        "max_date=df.agg({\"TotalCharges\":\"max\"})\n",
        "max_date.show()"
      ],
      "metadata": {
        "id": "Jtxup_GZKdVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Map and FlatMapflatmap splits each record by space in an RDD and finally flattens it. Resulting RDD consists of a single word on each record.The map function iterates over every line in RDD and split into new RDD. Using map() transformation we take in any function, and that function is applied to every element of RDD.\n",
        "\n",
        "df.rdd.map(lambda line: line.StreamingTV  .split(\" \")).take(50)"
      ],
      "metadata": {
        "id": "ynH7ngajMfPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.flatMap(lambda line: line.StreamingTV  .split(\" \")).take(50)"
      ],
      "metadata": {
        "id": "cGhJ0yMLMkq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "mGZLjf2aMr3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Startswith: Using Startswith to show records that start with F from the internetservice column\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "new_df=df.filter(col(\"InternetService\").startswith(\"F\")).show()"
      ],
      "metadata": {
        "id": "-9uXTSOPMzlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#groupby: Here, we are doing a groupBy on the Paymentmethod column and then finding the sum of Tenure for each paymentMethod using sum function\n",
        "\n",
        "df.groupBy(\"PaymentMethod\").sum(\"Tenure\").show(truncate=False)"
      ],
      "metadata": {
        "id": "lRO8jM5jM53I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}